{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载训练好的词典wordList和词典向量wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入word列表\n",
      "载入文本向量\n",
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('载入word列表')\n",
    "wordsList = wordsList.tolist()\n",
    "wordsList = [word.decode('UTF-8')\n",
    "             for word in wordsList]\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print('载入文本向量')\n",
    "\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList.index('unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.3302e-01,  4.5885e-01,  2.6602e-01,  3.8204e-02,  5.6694e-01,\n",
       "       -5.1194e-01, -1.7831e+00,  4.3072e-01,  1.3246e-03,  1.7852e-02,\n",
       "       -5.1549e-01, -5.4466e-01, -2.5115e-01, -3.7632e-01,  4.0969e-01,\n",
       "        7.0356e-02, -2.6120e-01, -2.7769e-03, -5.1361e-01,  2.9828e-01,\n",
       "        1.4496e-01,  5.3748e-01, -6.2443e-01,  4.1379e-01, -2.2006e-01,\n",
       "       -1.6277e+00,  2.9026e-01, -1.8530e-02,  5.1004e-01, -6.1231e-01,\n",
       "        3.2380e+00,  7.4873e-01,  1.8835e-01,  2.2167e-02,  5.7824e-01,\n",
       "        5.2022e-01,  1.5287e-01,  4.1601e-01,  5.5260e-01, -4.0373e-01,\n",
       "       -1.8057e-01,  2.0914e-01, -2.3873e-01,  1.1284e-01, -7.4940e-03,\n",
       "        5.7918e-01, -4.3641e-01, -8.0904e-01,  2.0410e-01, -1.9532e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_index = wordsList.index('home')\n",
    "wordVectors[home_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245\n"
     ]
    }
   ],
   "source": [
    "def get_content(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = ''\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "        return content\n",
    "\n",
    "import glob\n",
    "files = glob.glob('./pos/*.txt')\n",
    "corpus = [get_content(x) for x in files][0:2245]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载要分析情感的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面评价完结\n",
      "负面评价完结\n",
      "文件总数 4490\n",
      "所有的词的数量 1056147\n",
      "平均文件词的长度 235.22204899777282\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "# pos_files = ['pos/' + f for f in os.listdir(\n",
    "#     'pos/') if isfile(join('pos/', f))]\n",
    "neg_files = ['neg/' + f for f in os.listdir(\n",
    "    'neg/') if isfile(join('neg/', f))]\n",
    "num_words = []\n",
    "for pf in corpus:\n",
    "    line = pf\n",
    "    counter = len(line.split())\n",
    "    num_words.append(counter)\n",
    "print('正面评价完结')\n",
    "\n",
    "for nf in neg_files:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        num_words.append(counter)\n",
    "print('负面评价完结')\n",
    "\n",
    "num_files = len(num_words)\n",
    "print('文件总数', num_files)\n",
    "print('所有的词的数量', sum(num_words))\n",
    "print('平均文件词的长度', sum(num_words) / len(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 查看文本包含的词数范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujiaocan/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('qt4agg')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.pyplot.hist(num_words, 50, facecolor='g')\n",
    "matplotlib.pyplot.xlabel('Length of text')\n",
    "matplotlib.pyplot.ylabel('Frequence')\n",
    "matplotlib.pyplot.axis([0, 1200, 0, 1000])\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 将文本转化成 文本数 * 文本内的单词数 大小的索引矩阵\n",
    "#### 分析上述直方图，发现文本单词数设为300时能覆盖大大部分的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "将文本转化成一个索引矩阵 4490 * 300\n",
    "'''\n",
    "\n",
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "\n",
    "max_seq_num = 300\n",
    "ids = np.zeros((num_files, max_seq_num), dtype='int32')\n",
    "file_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4490, 300)\n"
     ]
    }
   ],
   "source": [
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pf in corpus:\n",
    "    line = pf\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter < max_seq_num and file_count < num_files:\n",
    "            try:\n",
    "                ids[file_count][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[file_count][indexCounter] = 399999\n",
    "            indexCounter = indexCounter + 1\n",
    "    file_count = file_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   83    81   596 ...     0     0     0]\n",
      " [19918    31    51 ...     4    71 11335]\n",
      " [  192   311    14 ...  2909  9703     6]\n",
      " ...\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245\n"
     ]
    }
   ],
   "source": [
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg/7910_2.txt\n"
     ]
    }
   ],
   "source": [
    "for nf in neg_files:\n",
    "    with open(nf, \"r\",encoding='utf-8') as f:\n",
    "        indexCounter = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        if len(split)==0:\n",
    "            print(nf)\n",
    "        else:\n",
    "            for word in split:\n",
    "                if indexCounter < max_seq_num and file_count < num_files:\n",
    "                    try:\n",
    "                        ids[file_count][indexCounter] = wordsList.index(word)\n",
    "                    except ValueError:\n",
    "                        ids[file_count][indexCounter] = 399999  # 未知的词语\n",
    "                    indexCounter = indexCounter + 1\n",
    "    file_count = file_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "batch_size = 24\n",
    "lstm_units = 64\n",
    "num_labels = 2\n",
    "iterations = 100\n",
    "lr = 0.001\n",
    "ids = np.load('idsMatrix.npy')\n",
    "\n",
    "'''\n",
    "    辅助函数，返回一个数据集的迭代器，用于返回一批训练集合\n",
    "'''\n",
    "\n",
    "def get_train_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in rang(batch_size):\n",
    "        if (i%2==0):\n",
    "            num = randint(1, 11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499, 24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1 :num]\n",
    "    return arr, labels\n",
    "\n",
    "def get_test_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in range(batch_size):\n",
    "        num = randint(11499, 13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            labels.append([0, 1])\n",
    "        arr[i] = ids[num - 1:num]\n",
    "    return arr, labels\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batch_size, num_labels])\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, max_seq_num])\n",
    "data = tf.Variable(tf.zeros([batch_size, max_seq_num, num_dimensions]), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.5)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstm_units, num_labels]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.exists(\"models\") and os.path.exists(\"models/checkpoint\"):\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "    else:\n",
    "        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "            init = tf.initialize_all_variables()\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    iterations = 100\n",
    "    for step in range(iterations):\n",
    "        next_batch, next_batch_labels = get_test_batch()\n",
    "        if step % 20 == 0:\n",
    "            print(\"step:\", step, \" 正确率:\", (sess.run(\n",
    "                accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100)\n",
    "\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
