{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一刷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于规则的分词：正/逆/双向最大匹配，依赖词典，新出现的词（未登陆词）可能无法分出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南京市', '长江大桥']\n"
     ]
    }
   ],
   "source": [
    "# 逆向最大匹配\n",
    "class IMM(object):\n",
    "    \n",
    "    def __init__(self, dic_path):\n",
    "        self.dictionary = set() # 集合，添加用add()方法\n",
    "        self.maximun = 0\n",
    "        # 读取词典\n",
    "        with open(dic_path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                if len(line) > self.maximun:\n",
    "                    self.maximun = len(line)\n",
    "                    \n",
    "    def cut(self, text):\n",
    "        result = []\n",
    "        index = len(text)  # 获取当前text的长度，从后往前定位到最后一个字\n",
    "        while index > 0:\n",
    "            word = None    # 一开始word里是空的\n",
    "            for size in range(self.maximun, 0, -1):  # 逆向往前推\n",
    "                if index - size < 0:   # 按照词典里最长的词语的长度来首次匹配，若匹配不上，则退出\n",
    "                    continue\n",
    "                piece = text[(index-size): index] #  若逆向且最大长度的词语的长度符合要求\n",
    "                if piece in self.dictionary:      #  且该词语也存在词典中\n",
    "                    word = piece\n",
    "                    result.append(word)\n",
    "                    index -= size\n",
    "                    break\n",
    "            if word is None:\n",
    "                index -= 1\n",
    "        return result[::-1] # 逆向添加的结果，最后要逆向输出\n",
    "\n",
    "def main():\n",
    "    text = \"南京市长江大桥\"\n",
    "    tokenizer = IMM(\"./chapter3/imm_dic.utf8\")\n",
    "    print(tokenizer.cut(text))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    词典imm_dic.utf8中的内容\n",
    "'''\n",
    "# imm_dic.utf8 \n",
    "# 南京市\n",
    "# 南京市长\n",
    "# 长江大桥\n",
    "# 人名解放军\n",
    "# 大桥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于统计的分词 HMM CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合分词：规则＋统计，如jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高频词提取1\n",
    "\n",
    "def get_content(path):\n",
    "    with open(path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "        content = ''\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "        return content\n",
    "\n",
    "def get_TF(words, topK=10):\n",
    "    tf_dic = {}\n",
    "    for w in words:\n",
    "        tf_dic[w] = tf_dic.get(w, 0) + 1\n",
    "#         print(tf_dic)\n",
    "    return sorted(tf_dic.items(), key=lambda x: x[1], reverse=True)[:topK]\n",
    "\n",
    "def stop_words(path):\n",
    "    with open(path) as f:\n",
    "        return [l.strip() for l in f]\n",
    "    \n",
    "# stop_words('./chapter3/stop_words.utf8')\n",
    "\n",
    "# 分词\n",
    "# def main():\n",
    "#     import glob\n",
    "#     import random\n",
    "#     import jieba\n",
    "    \n",
    "#     files = glob.glob(\"./chapter3/data/C000013/*.txt\") # 查找符合特定规则的文件路径名, \"*\"匹配0个或多个字符；\"?\"匹配单个字符；\"[]\"匹配指定范围内的字符,如：[0-9]匹配数字\n",
    "#     corpus = [get_content(x) for x in files[:5]]\n",
    "# #     print(files[:5])\n",
    "# #     print(corpus)\n",
    "    \n",
    "#     sample_inx = random.randint(0, len(corpus))\n",
    "# #     print(len(corpus))\n",
    "# #     print(sample_inx)\n",
    "\n",
    "    \n",
    "#     import jieba.posseg as psg\n",
    "    \n",
    "#     split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./chapter3/stop_words.utf8')]\n",
    "    \n",
    "# #     print(corpus[sample_inx])\n",
    "    \n",
    "#     print('样本之一：'+ corpus[sample_inx])\n",
    "#     print('样本分词效果：'+'/ '.join(split_words))\n",
    "#     print('样本的topK（10）词：'+ str(get_TF(split_words)))\n",
    "\n",
    "# main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "6\n",
      "['日本', '医科大学', '医院', '疫苗', '方法', '乙型肝炎', '母婴', '方法', '次数', '效果', '优点', '日本', '经济', '新闻', '医院', '院长', '稻叶', '宪', '新', '方法', '婴儿', '药物', '婴儿', '小时', '乙肝', '专题', '疫苗', '婴儿', '健康检查', '时', '疫苗', '证实', '方法', '婴儿', '免疫力', '乙型肝炎', '肝硬化', '肝癌', '专题', '血液', '母婴', '传统', '疫苗', '病毒', '母亲', '新生儿', '次数', '周期长', '婴儿', '现象', '乙肝', '母婴', '病例', '中约', '疫苗']\n",
      "***********************************************************************************\n",
      "['ns', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'n', 'n', 'n', 'n', 'n', 'nr', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'n']\n",
      "样本之一：日本独协医科大学医院开发一种新的疫苗接种方法，可有效阻断乙型肝炎在母婴间的传播。新方法具有接种次数少、效果好的优点。据《日本经济新闻》报道，这家医院的院长稻叶宪之等人开发的新方法分两次给婴儿注射药物，第一次是在婴儿出生24小时内立即注射球蛋白和乙肝(专题 访谈 咨询)疫苗，第二次是在婴儿出生一个月后接受健康检查时，再注射一次疫苗。临床研究证实，这种方法可使婴儿获得足够的免疫力。乙型肝炎可发展为肝硬化及肝癌(专题 访谈 咨询)等。它通常经血液传播或在母婴间传播。传统的疫苗接种法虽然也能阻断病毒在母亲和新生儿之间传播，但接种次数多，且周期长，因而经常出现婴儿未及时接种的现象。调查显示，乙肝在母婴间传播的病例中约有30％是因未按规定及时注射疫苗引起的。\n",
      "样本分词效果：日本/ 医科大学/ 医院/ 疫苗/ 方法/ 乙型肝炎/ 母婴/ 方法/ 次数/ 效果/ 优点/ 日本/ 经济/ 新闻/ 医院/ 院长/ 稻叶/ 宪/ 新/ 方法/ 婴儿/ 药物/ 婴儿/ 小时/ 乙肝/ 专题/ 疫苗/ 婴儿/ 健康检查/ 时/ 疫苗/ 证实/ 方法/ 婴儿/ 免疫力/ 乙型肝炎/ 肝硬化/ 肝癌/ 专题/ 血液/ 母婴/ 传统/ 疫苗/ 病毒/ 母亲/ 新生儿/ 次数/ 周期长/ 婴儿/ 现象/ 乙肝/ 母婴/ 病例/ 中约/ 疫苗\n",
      "样本的topK（10）词：[('疫苗', 5), ('婴儿', 5), ('方法', 4), ('母婴', 3), ('日本', 2), ('医院', 2), ('乙型肝炎', 2), ('次数', 2), ('乙肝', 2), ('专题', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#高频词提取2\n",
    "\n",
    "def main():\n",
    "    import glob\n",
    "    import random\n",
    "    import jieba\n",
    "    import jieba.posseg as psg\n",
    "    \n",
    "    files = glob.glob('./chapter3/data/C000013/*.txt')\n",
    "    corpus = [get_content(x) for x in files]  # 若不切片，就读取设置路径下的全部文件\n",
    "    print(len(corpus))   # 15\n",
    "#     print(corpus)\n",
    "    \n",
    "    sample_inx = random.randint(0, len(corpus))\n",
    "    print(sample_inx)\n",
    "#     sample_inx = 3\n",
    "    \n",
    "    aa = [w for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    \n",
    "    bb = [t for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    \n",
    "    print(aa)\n",
    "    print(\"***********************************************************************************\")\n",
    "    print(bb)\n",
    "    \n",
    "    split_words = [w for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    print('样本之一：' + corpus[sample_inx])\n",
    "    print('样本分词效果：' + '/ '.join(split_words))\n",
    "    print('样本的topK（10）词：'+ str(get_TF(split_words)))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文/nz 分词/n 是/v 文本处理/n 不可或缺/l 的/uj 一步/m ！/x\n"
     ]
    }
   ],
   "source": [
    "# 练习jieba的使用\n",
    "\n",
    "import jieba.posseg as psg\n",
    "\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "\n",
    "seg_list = psg.cut(sent)\n",
    "\n",
    "print(' '.join(['{0}/{1}'.format(w, t) for w, t in seg_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大波浪 10\n",
      "\n",
      "jieba分词 n\n",
      "\n",
      "金融词典 7  \n",
      "\n",
      "加载词典前: jieba分词/ 非常/ 好用/ ，/ 可以/ 自定义/ 金融词典/ ！\n",
      "加载词典后: jieba分词/ 非常/ 好用/ ，/ 可以/ 自定义/ 金融词典/ ！\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('./chapter3/dict.txt.big') # 改变主词典的路径，默认用的是jieba的分词词典\n",
    "\n",
    "with open('./chapter3/user_dict.utf8', 'r') as f:\n",
    "    for l in f:\n",
    "        print(l)\n",
    "        \n",
    "sent = 'jieba分词非常好用，可以自定义金融词典！'\n",
    "seg_list = jieba.cut(sent)\n",
    "print('加载词典前:', '/ '.join(seg_list))\n",
    "\n",
    "jieba.load_userdict('./chapter3/user_dict.utf8') # 添加一些用户自定义的词典\n",
    "seg_list = jieba.cut(sent)\n",
    "print('加载词典后:', '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好丑/ 的/ 证件/ 照片\n",
      "好丑/ 的/ 证件照片\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "sent = '好丑的证件照片'\n",
    "print('/ '.join(jieba.cut(sent, HMM=False)))\n",
    "\n",
    "jieba.suggest_freq(('证件照片'), True)\n",
    "print('/ '.join(jieba.cut(sent, HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言 2.0790900005043476\n",
      "NLP 0.5197725001260869\n",
      "计算机 0.5197725001260869\n",
      "领域 0.5197725001260869\n",
      "人机交互 0.5197725001260869\n",
      "挑战 0.5197725001260869\n",
      "理解 0.5197725001260869\n",
      "处理 0.4705091875965217\n",
      "涉及 0.3839134341652174\n",
      "人工智能 0.25988625006304344\n"
     ]
    }
   ],
   "source": [
    "# 基于 TF-IDF 算法的关键词抽取 \n",
    "# jieba.analyse.extract_tags()\n",
    "\n",
    "import jieba.analyse as aly\n",
    "\n",
    "content = '''\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "\n",
    "'''\n",
    "#加载自定义idf词典\n",
    "aly.set_idf_path('./chapter3/idf.txt.big')\n",
    "#加载停用词典\n",
    "aly.set_stop_words('./chapter3/stop_words.utf8')\n",
    "\n",
    "# 第一个参数：待提取关键词的文本\n",
    "# 第二个参数：返回关键词的数量，重要性从高到低排序\n",
    "# 第三个参数：是否同时返回每个关键词的权重\n",
    "# 第四个参数：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词\n",
    "keywords = aly.extract_tags(content, topK=10, withWeight=True, allowPOS=())\n",
    "\n",
    "for item in keywords:\n",
    "    # 分别为关键词和相应的权重\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "涉及 1.0\n",
      "计算机 0.9618169519358478\n",
      "处理 0.8124660402732825\n",
      "理解 0.7885898958379202\n",
      "挑战 0.7833575495518058\n",
      "人机交互 0.7343470452632993\n",
      "语言学 0.727536034596871\n",
      "人类 0.6290562193534068\n",
      "人工智能 0.5809911385488661\n",
      "关注 0.577881611632419\n"
     ]
    }
   ],
   "source": [
    "# 基于 TextRank 算法的关键词抽取\n",
    "# jieba.analyse.TextRank() \n",
    "import jieba.analyse as aly\n",
    "\n",
    "content = '''\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "'''\n",
    "\n",
    "# 第一个参数：待提取关键词的文本\n",
    "# 第二个参数：返回关键词的数量，重要性从高到低排序\n",
    "# 第三个参数：是否同时返回每个关键词的权重\n",
    "# 第四个参数：词性过滤，为空表示过滤所有，与TF—IDF不一样！\n",
    "\n",
    "keywords = jieba.analyse.textrank(content, topK=10, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "for item in keywords:\n",
    "    # 分别为关键词和相应的权重\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二刷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(path):\n",
    "    with open(path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "        content = ''\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF(words, topK=10):\n",
    "    tf_dic = {}\n",
    "    for w in words:\n",
    "        tf_dic[w] = tf_dic.get(w, 0) + 1\n",
    "    return sorted(tf_dic.items(), key=lambda x:x[1], reverse=True)[:topK] # 按照词典的values值的大小降序处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(path):\n",
    "    with open(path) as f:\n",
    "        return [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "import jieba\n",
    "files = glob.glob('./data/C000013/*.txt')\n",
    "corpus = [get_content(x) for x in files]\n",
    "sample_inx = random.randint(0, len(corpus))\n",
    "print(sample_inx)\n",
    "# split_words = [word for word in jieba.cut(corpus[sample_inx])] # 等价于 list(jieba.cut(corpus[sample_inx]))\n",
    "split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./stop_words.utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['中国', '药科', '大学', '国际', '医药', '商学院', '副', '院长', '、', '博士生', '导师', '顾海', '从', '四个', '方面', '分析', '了', '泰州', '做', '大', '做', '强', '医药产业', '的', '优势', '。', '产业', '基础', '雄厚', '。', '2004', '年', '全市', '医药产业', '11', '个', '重点', '产品', '单品', '销售', '过', '亿元', '，', '47', '个', '销售', '过', '千万元', '，', '10', '个', '产品', '夺得', '全国', '医药', '市场', '销售', '单项', '冠军', '。', '截止', '到', '2004', '年底', '，', '扬子江', '、', '江山', '、', '济川', '、', '苏中', '制药', '成为', '全国', '医药', '百强', '企业', '。', '另外', '，', '凯华', '集团', '、', '凯泰', '集团', '等', '一批', '成长性', '较', '好', '的', '重点', '药包材', '、', '医疗器械', '生产', '企业', '也', '在', '迅速', '发展', '。', '研发', '能力', '和', '产业化', '能力', '强', '，', '全市', '医药企业', '现在', '有', '博士后', '工作站', '2', '个', '，', '药物', '研究', '机构', '7', '家', '、', '研发', '人员', '200', '余人', '。', '2000', '年', '以来', '，', '有', '13', '家', '企业', '的', '79', '个', '剂型', '通过', 'GMP', '认证', '。', '目前', '，', '全市', '98%', '的', '药品', '是从', 'GMP', '车间', '生产', '出来', '的', '，', '产品', '抽样合格率', '提高', '到', '了', '97%', '以上', '。', '首批', '国家级', '医药', '出口', '基地', '。', '今年', '上半年', '，', '泰州', '被', '商务部', '确认', '为', '首批', '国家级', '医药', '出口', '基地', '，', '这为', '泰州', '医药产业', '加速', '进军', '国际', '市场', '，', '拓展', '市场', '空间', '带来', '重大', '机遇', '。', '区位', '优势', '明显', '。', '泰州市', '地处', '长江下游', '，', '交通', '便利', '，', '基础设施', '齐全', '，', '产业', '基础', '扎实', '，', '是', '长三角', '经济区', '和', '沿江', '开发', '的', '重要', '板块', '。', '泰州', '打造', '医药', '城', '，', '通过', '医药', '产业园', '建设', '，', '加大', '对', '医药产业', '的', '招商', '力度', '，', '将', '有利于', '国外', '和', '周边地区', '医药企业', '在', '泰州', '形成', '企业', '群', '。', '（', '信息', '来源', '：', '泰州', '网', '）']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./data/C000013/*.txt')\n",
    "corpus = [get_content(x) for x in files]\n",
    "sample_inx = random.randint(0, len(corpus))\n",
    "print(sample_inx)\n",
    "aa = jieba.lcut(corpus[sample_inx])\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "都说“幸福象花儿一样”，你可知道有的花虽然娇艳无比惹人怜爱，却也是诱发多种疾病的“祸水”呢！一、可能引起失眠的花1兰花：它的香气会引起失眠。2百合花：它的香味也会使人的中枢神经过度兴奋而引起失眠。如果失眠了怎么办？有一种“不睡觉”法很好用哦！点击进入：歼灭失眠的六大法宝！二、可能使毛发脱落的花1含羞草：它的体内含草碱能使毛发脱落。2郁金香：它的花朵含有一种毒碱，接触过久，会加快毛发脱落。拥有一头浓密有光泽的头发是多么幸福的事啊！想象一下如果没有头发，人生将会怎样！怎样预防脱发？教你防脱发10大秘诀三、可能诱发加重呼吸道疾病的花1紫荆花：它的花粉会诱发哮喘症或使咳嗽症状加重。2月季花：它的浓郁香味，会使一些人产生胸闷不适，憋气与呼吸困难。哮喘是一种相当多见的呼吸道疾病，病因较为复杂，治疗上至今还没有突破性进展。寻找哮喘反复发作的原因，以便采取相应措施，对控制病情至关重要。点击进入：对于哮喘，你是否存在以下七大问题？四、可能引发过敏的花洋绣球花：它所散发的微粒，会使人皮肤过敏而引发瘙痒症。此外，吃野菜也容易引发过敏，如野芹菜、野葱、莼菜、灰菜、马齿苋、槐花、野生小蒜等，都含有可导致过敏的物质，容易在某些特殊体质的人身上，引起过敏反应。例如灰菜、马齿苋、槐花等，吃后皮肤在太阳下暴露，极易诱发日光性皮炎，在眼睑、面部、颈部及手臂等部位出现红肿、瘙痒等症状，甚至出现水疱。野生小蒜如果吃多了，会出现咽喉干燥、眼目赤肿等症状。过敏了，有人皮肤起斑、喷嚏不断；有人眼睛发痒、哮喘连连，专家提醒说，尤其要警惕五种过敏疾病！五、对胃肠道有刺激的花木松柏：松柏类花木的芳香气味对人体的肠胃有刺激作用。胃肠功能弱的人应该避免长期在松柏林中活动，以免引发胃肠不适。六、可能使人头晕的花夜来香：它的香气会使高血压和心脏病患者感到头晕目眩郁闷不适。所以高血压和心脏病患者应该把夜来香搬离房间。七、可能引起中毒的花1夹竹桃：它的分泌液会使人中毒，引起昏昏欲睡，智力下降。2黄花杜鹃：它的花朵含有一种毒素，一旦误食，轻者会引起中毒，重者会引起休克。\n"
     ]
    }
   ],
   "source": [
    "print(corpus[sample_inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水母网/4/月/13/日讯/（/记者/闫晓智/实习生/田露/摄影/报道/）/家住/莱山区/的/俞/女士/致电/党报/热线/6601234/,/不满/地向/记者/“/投诉/”/：/“/现在/超市/的/促销/手法/太高明/了/，/对/快要/到/保质期/的/食品/实行/‘/捆绑/销售/’/，/消费者/稍不留神/就/会/吃哑巴亏/”/。/据/俞/女士/讲/，/9/日/傍晚/，/她/在/小/区内/的/便民/超市/花/46/元/钱/买/了/一桶/某/品牌/正在/促销/的/花生油/。/看到/5/公斤/的/大桶/上/还/用/透明胶/带/“/捆绑/”/了/一小/瓶/香油/，/感觉/特别/划算/，/就/买/了/一桶/。/谁知/，/昨晚/做饭/时/她/才/无意/中/发现/，/大桶/花生油/离/保质期/只/剩下/21/天/。/“/我们/家/只有/3/口人/，/5/公斤/花生油/怎么/可能/在/20/天内/吃/完/？/”/俞/女士/显得/无可奈何/。/记者/从/一些/超市/了解/到/，/像/俞/女士/遭遇/的/这种/“/捆绑/”/促销/方式/在/很多/超市/中/都/颇/盛行/，/促销/的/商品/大多/集中/在/牛奶/、/酸奶/、/方便面/等/食品/上/。/很多/消费者/在/购买/此类/食品/时/，/往往/因为/买到/了/“/便宜/”/而/非常/开心/，/忽略/了/生产日期/，/等/过些天/想/起来/食用/时/，/才/发现/已经/过期/了/。/一位/张女士/告诉/记者/，/她/就/曾/吃/过/这种/“/捆绑/”/促销/牛奶/的/亏/。/当时/，/她/在/一家/超市/中/看到/某/品牌/牛奶/“/买五赠/一/”/，/因为/透明胶/带/“/捆绑/”/得/非常/结实/，/她/只/看到/了/最/外面/一袋/牛奶/的/生产日期/是/前一天/的/，/按照/一个多月/的/保质期/推算/，/就/放心/地买/回家/了/。/回家/后/，/张女士/就/把/牛奶/放在/冰箱/里/，/3/天后/打开/捆绑/牛奶/的/胶带/，/才/发现/其中/两袋/已经/过期/一两天/了/。/“/幸亏/我/又/看/了/一眼/，/不然/喝/了/可/怎么办/？/”/当/记者/向/商家/问及/为什么/采取/“/捆绑式/”/销售/快/过期/食品/时/，/一家/小型/超市/的/负责人/说/，/促销/“/顶期/”/（/离/保质期/很/短时间/的/）/食品/是/很/正常/的/事/。/商品/一旦/过期/没/卖出去/，/商家/就要/赔/很多/钱/，/所以/干脆/趁着/还/没有/过期/，/作为/赠品/与/其他/商品/一起/出售/，/这样/能/把/损失/降到/最低/。/商家/的/想法/听/起来/似乎/可以/理解/，/但/事实上/这种/变相/遮挡/生产日期/捆绑/销售/“/顶期/食品/”/的/做法/在/某种程度/上/已经/侵害/了/消费者/的/知情权/。/记者/从市/消协/了解/到/，/《/产品质量法/》/、/《/食品/卫生法/》/等/相关/法律法规/只/规定/不能/销售/过期/食品/，/并未/有/不能/销售/“/顶期/食品/”/的/规定/。/消协/工作人员/提醒/消费者/，/保质期/内/的/食品/，/在/口感/、/营养/等/方面/都/是/最好/的/。/而/过/了/保质期/，/虽然/食品/外表/看起来/可能/没有/什么/变化/，/但/食品/的/基本/品质/已经/有所/改变/，/时间/长/了/连/卫生指标/也/会/起/变化/。/因此/，/消费者/在/购买/“/捆绑/”/促销/的/“/顶期/食品/”/时/，/一定/要/先/看清/保质期/，/并/保留/凭证/，/发现/问题/及时/投诉/。\n"
     ]
    }
   ],
   "source": [
    "print(\"/\".join(split_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('，', 43), ('的', 24), ('。', 17), ('“', 16), ('”', 16), ('了', 15), ('食品', 13), ('在', 9), ('捆绑', 8), ('促销', 7)]\n"
     ]
    }
   ],
   "source": [
    "print(get_TF(split_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('会', 11), ('花', 8), ('引起', 8), ('可能', 6), ('失眠', 5), ('哮喘', 5), ('过敏', 5), ('诱发', 4), ('疾病', 4), ('一种', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(get_TF(split_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as psg\n",
    "\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "\n",
    "seg_list = psg.cut(sent)\n",
    "\n",
    "# print(' '.join(['{0}/{1}'.format(w, t) for w, t in seg_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['{0}/{1}'.format(w, t) for w, t in seg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中文/nz', '分词/n', '是/v', '文本处理/n', '不可或缺/l', '的/uj', '一步/m', '！/x']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word, label = [], []\n",
    "for pair in result:\n",
    "    word.append(pair.split(\"/\")[0])\n",
    "    label.append(pair.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中文', '分词', '是', '文本处理', '不可或缺', '的', '一步', '！']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nz', 'n', 'v', 'n', 'l', 'uj', 'm', 'x']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('自然语言', 1.669590679872),\n",
       " ('NLP', 0.47819070011599996),\n",
       " ('人机交互', 0.456630840088),\n",
       " ('处理', 0.4328684525888),\n",
       " ('涉及', 0.353200359432),\n",
       " ('计算机', 0.2721913772352),\n",
       " ('挑战', 0.26371219484840003),\n",
       " ('理解', 0.24174335148280002),\n",
       " ('领域', 0.2164918216964),\n",
       " ('计算机科学', 0.1951508585116)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse as aly\n",
    "\n",
    "content = '''\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "'''\n",
    "aly.extract_tags(content, topK=10, withWeight=True, allowPOS=(),withFlag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(pair('计算机', 'n'), 1.0),\n",
       " (pair('语言学', 'n'), 0.9611979789581414),\n",
       " (pair('人类', 'n'), 0.781851411703657),\n",
       " (pair('人工智能', 'n'), 0.7193992641629281),\n",
       " (pair('计算机科学', 'n'), 0.4952296819743204),\n",
       " (pair('语言', 'n'), 0.30470045351032227),\n",
       " (pair('人为', 'n'), 0.29023283822240514)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.textrank(content, topK=10, withWeight=True, allowPOS=([\"n\"]),withFlag=True)\n",
    "# jieba.analyse.extract_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('自然语言', 8), ('，', 8), ('的', 6), ('\\n', 4), ('处理', 4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(jieba.lcut(content)).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "?jieba.analyse.textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
