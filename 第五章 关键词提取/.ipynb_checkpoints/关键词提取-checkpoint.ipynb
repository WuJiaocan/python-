{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一刷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词表加载方法\n",
    "def get_stopword_list():\n",
    "    # 停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    # 进行编码转换确保匹配准确率\n",
    "    stop_word_path = './stopword.txt'\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path).readlines()] \n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词方法，调用结巴接口\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除干扰词\n",
    "def word_filter(seg_list, pos=False):\n",
    "    stopword_list = get_stopword_list()\n",
    "    filter_list = []\n",
    "    # 根据POS参数选择是否词性过滤\n",
    "    ## 不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'): # 通过词性查看，如果不是名词类的词，则舍弃。\n",
    "            continue\n",
    "        # 过滤停用词表中的词，以及长度为<2的词\n",
    "        if word not in stopword_list and len(word) > 1:\n",
    "            filter_list.append(word)\n",
    "\n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pos=False, corpus_path='./corpus.txt'):\n",
    "    # 调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    doc_list = []\n",
    "    for line in open(corpus_path, 'r'):\n",
    "        content = line.strip()\n",
    "        seg_list = seg_to_list(content, pos)\n",
    "        filter_list = word_filter(seg_list, pos)\n",
    "        doc_list.append(filter_list)\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf/idf算法\n",
    "#### 需要训练语料库，对模型的参数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_idf(doc_list):\n",
    "    idf_dic={}\n",
    "    # 总文档数\n",
    "    tt_count = len(doc_list)\n",
    "    \n",
    "    # 在整个文档集中，每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "    \n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k,v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "        \n",
    "    # 对于没有在字典中的字，默认其仅在一个文档中出现，得到默认的idf值\n",
    "    default_idf = math.log(tt_count / 1.0)\n",
    "    \n",
    "    return idf_dic, default_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  排序函数，用于topK关键词的按值排序\n",
    "def cmp(e1, e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1]) # np.sign 就是大于0的返回1.0, 小于0的返回-1.0, 等于0的返回0.0\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a ==b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    # 统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    # 按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        # key=functools.cmp_to_key(cmp)\n",
    "        for k, v in sorted(tfidf_dic.items(), key=lambda x: x[1], reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textrank算法\n",
    "#### 不用训练语料库，直接根据单个文档就可以对关键词进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/ \", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主题模型，LSA(LSI) / LDA\n",
    "#### 需要训练语料库，对模型的参数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载的模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "\n",
    "        dictionary = list(set(dictionary))\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "    topic_model.get_simword(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['年度', '中国', '爱心', '城市', '公益活动', '新闻', '发布会', '在京举行', '中华', '社会', '基金会', '理事长', '许嘉璐', '讲话', '基金会', '朱发忠', '全国', '老龄', '朱勇', '民政部', '社会', '巡视员', '周萍', '中华', '社会', '基金会', '副理事长', '志远', '重庆市', '民政局', '巡视员', '明政', '晋江市', '人大常委会', '陈健倩', '自治区', '民政局', '领导', '媒体', '发布会', '中华', '社会', '基金会', '秘书长', '本年度', '中国', '爱心', '城市', '公益活动', '爱心', '城市', '项目', '中国', '爱心', '城市', '大会', '内容', '重庆市', '呼和浩特市', '长沙市', '太原市', '蚌埠市', '南昌市', '汕头市', '沧州市', '晋江市', '遵化市', '公益活动', '中国', '雅虎', '副总编', '张银生', '凤凰网', '城市', '频道', '总监', '赵耀', '媒体', '优势', '方案', '中华', '社会', '基金会', '中国', '爱心', '城市', '大会', '承办方', '晋江市', '许嘉璐', '理事长', '晋江市', '国家', '重点', '地区', '价值', '款物', '晋江市', '人大常委会', '陈健倩', '大会', '情况']\n",
      "TF-IDF模型结果：\n",
      "晋江市/ 城市/ 大会/ 爱心/ 中华/ 基金会/ 许嘉璐/ 巡视员/ 重庆市/ 人大常委会/ \n",
      "TextRank模型结果：\n",
      "城市/ 爱心/ 救助/ 中国/ 社会/ 晋江市/ 基金会/ 大会/ 介绍/ 公益活动/ \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "           '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "    \n",
    "    print(filter_list)\n",
    "    \n",
    "    print('TF-IDF模型结果：')\n",
    "#     analyse.extract_tags(filter_list)\n",
    "    tfidf_extract(filter_list)\n",
    "    \n",
    "    print('TextRank模型结果：')\n",
    "    textrank_extract(text)\n",
    "    \n",
    "#     print('LSI模型结果：')\n",
    "#     topic_extract(filter_list, 'LSI', pos)\n",
    "    \n",
    "#     print('LDA模型结果：')\n",
    "#     topic_extract(filter_list, 'LDA', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二刷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from jieba import analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    stop_word_path = \"./stopword.txt\"\n",
    "    stopword_list = [sw.replace(\"\\n\", \"\") for sw in open(stop_word_path).readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(seg_list, pos=False):\n",
    "    stopword_list = get_stopword_list()\n",
    "    filter_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "       '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "       '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "       '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "       '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "       '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "       '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "       '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "       '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "       '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "       '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "\n",
    "seg_list = psg.cut(text)\n",
    "\n",
    "stop_word_path = './stopword.txt'\n",
    "stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path).readlines()] \n",
    "\n",
    "filter_list = []\n",
    "for seg in seg_list:\n",
    "    word = seg.word\n",
    "    flag = seg.flag\n",
    "    if not flag.startswith('n'): # 通过词性查看，如果不是名词类的词，则舍弃。\n",
    "        continue\n",
    "#     # 过滤停用词表中的词，以及长度为<2的词\n",
    "    if word not in stopword_list and len(word) > 1:\n",
    "        filter_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['年度', '中国', '爱心', '城市', '公益活动', '新闻', '发布会', '在京举行', '中华', '社会', '基金会', '理事长', '许嘉璐', '讲话', '基金会', '朱发忠', '全国', '老龄', '朱勇', '民政部', '社会', '巡视员', '周萍', '中华', '社会', '基金会', '副理事长', '志远', '重庆市', '民政局', '巡视员', '明政', '晋江市', '人大常委会', '陈健倩', '自治区', '民政局', '领导', '媒体', '发布会', '中华', '社会', '基金会', '秘书长', '本年度', '中国', '爱心', '城市', '公益活动', '爱心', '城市', '项目', '中国', '爱心', '城市', '大会', '内容', '重庆市', '呼和浩特市', '长沙市', '太原市', '蚌埠市', '南昌市', '汕头市', '沧州市', '晋江市', '遵化市', '公益活动', '中国', '雅虎', '副总编', '张银生', '凤凰网', '城市', '频道', '总监', '赵耀', '媒体', '优势', '方案', '中华', '社会', '基金会', '中国', '爱心', '城市', '大会', '承办方', '晋江市', '许嘉璐', '理事长', '晋江市', '国家', '重点', '地区', '价值', '款物', '晋江市', '人大常委会', '陈健倩', '大会', '情况']\n"
     ]
    }
   ],
   "source": [
    "print(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→', '℃ ', '&', '*', '一一', '~~~~', '’', '. ', '『', '.一', './', '-- ', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］', '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', '［', '］', '［②ｅ］', '［②ｇ］', '＝｛', '}', '，也 ', '‘', 'Ａ', '［①⑥］', '［②Ｂ］ ', '［①ａ］', '［④ａ］', '［①③］', '［③ｈ］', '③］', '１． ', '－－ ', '［②ｂ］', '’‘ ', '××× ', '［①⑧］', '０：２ ', '＝［', '［⑤ｂ］', '［②ｃ］ ', '［④ｂ］', '［②③］', '［③ａ］', '［④ｃ］', '［①⑤］', '［①⑦］', '［①ｇ］', '∈［ ', '［①⑨］', '［①④］', '［①ｃ］', '［②ｆ］', '［②⑧］', '［②①］', '［①Ｃ］', '［③ｃ］', '［③ｇ］', '［②⑤］', '［②②］', '一.', '［①ｈ］', '.数', '［］', '［①Ｂ］', '数/', '［①ｉ］', '［③ｅ］', '［①①］', '［④ｄ］', '［④ｅ］', '［③ｂ］', '［⑤ａ］', '［①Ａ］', '［②⑧］', '［②⑦］', '［①ｄ］', '［②ｊ］', '〕〔', '］［', '://', '′∈', '［②④', '［⑤ｅ］', '１２％', 'ｂ］', '...', '...................', '…………………………………………………③', 'ＺＸＦＩＴＬ', '［③Ｆ］', '」', '［①ｏ］', '］∧′＝［ ', '∪φ∈', '′｜', '｛－', '②ｃ', '｝', '［③①］', 'Ｒ．Ｌ．', '［①Ｅ］', 'Ψ', '－［＊］－', '↑', '.日 ', '［②ｄ］', '［②', '［②⑦］', '［②②］', '［③ｅ］', '［①ｉ］', '［①Ｂ］', '［①ｈ］', '［①ｄ］', '［①ｇ］', '［①②］', '［②ａ］', 'ｆ］', '［⑩］', 'ａ］', '［①ｅ］', '［②ｈ］', '［②⑥］', '［③ｄ］', '［②⑩］', 'ｅ］', '〉', '】', '元／吨', '［②⑩］', '２．３％', '５：０  ', '［①］', '::', '［②］', '［③］', '［④］', '［⑤］', '［⑥］', '［⑦］', '［⑧］', '［⑨］ ', '……', '——', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '．', ',', '．', \"'\", '? ', '·', '———', '──', '? ', '—', '<', '>', '（', '）', '〔', '〕', '[', ']', '(', ')', '-', '+', '～', '×', '／', '/', '①', '②', '③', '④', '⑤', '⑥', '⑦', '⑧', '⑨', '⑩', 'Ⅲ', 'В', '\"', ';', '#', '@', 'γ', 'μ', 'φ', 'φ．', '× ', 'Δ', '■', '▲', 'sub', 'exp ', 'sup', 'sub', 'Lex ', '＃', '％', '＆', '＇', '＋', '＋ξ', '＋＋', '－', '－β', '＜', '＜±', '＜Δ', '＜λ', '＜φ', '＜＜', '=', '＝', '＝☆', '＝－', '＞', '＞λ', '＿', '～±', '～＋', '［⑤ｆ］', '［⑤ｄ］', '［②ｉ］', '≈ ', '［②Ｇ］', '［①ｆ］', 'ＬＩ', '㈧ ', '［－', '......', '〉', '［③⑩］', '第二', '一番', '一直', '一个', '一些', '许多', '种', '有的是', '也就是说', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不仅', '不拘', '不论', '不怕', '不然', '不如', '不特', '不惟', '不问', '不只', '朝', '朝着', '趁', '趁着', '乘', '冲', '除', '除此之外', '除非', '除了', '此', '此间', '此外', '从', '从而', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话', '等', '等等', '地', '第', '叮咚', '对', '对于', '多', '多少', '而', '而况', '而且', '而是', '而外', '而言', '而已', '尔后', '反过来', '反过来说', '反之', '非但', '非徒', '否则', '嘎', '嘎登', '该', '赶', '个', '各', '各个', '各位', '各种', '各自', '给', '根据', '跟', '故', '故此', '固然', '关于', '管', '归', '果然', '果真', '过', '哈', '哈哈', '呵', '和', '何', '何处', '何况', '何时', '嘿', '哼', '哼唷', '呼哧', '乎', '哗', '还是', '还有', '换句话说', '换言之', '或', '或是', '或者', '极了', '及', '及其', '及至', '即', '即便', '即或', '即令', '即若', '即使', '几', '几时', '己', '既', '既然', '既是', '继而', '加之', '假如', '假若', '假使', '鉴于', '将', '较', '较之', '叫', '接着', '结果', '借', '紧接着', '进而', '尽', '尽管', '经', '经过', '就', '就是', '就是说', '据', '具体地说', '具体说来', '开始', '开外', '靠', '咳', '可', '可见', '可是', '可以', '况且', '啦', '来', '来着', '离', '例如', '哩', '连', '连同', '两者', '了', '临', '另', '另外', '另一方面', '论', '嘛', '吗', '慢说', '漫说', '冒', '么', '每', '每当', '们', '莫若', '某', '某个', '某些', '拿', '哪', '哪边', '哪儿', '哪个', '哪里', '哪年', '哪怕', '哪天', '哪些', '哪样', '那', '那边', '那儿', '那个', '那会儿', '那里', '那么', '那么些', '那么样', '那时', '那些', '那样', '乃', '乃至', '呢', '能', '你', '你们', '您', '宁', '宁可', '宁肯', '宁愿', '哦', '呕', '啪达', '旁人', '呸', '凭', '凭借', '其', '其次', '其二', '其他', '其它', '其一', '其余', '其中', '起', '起见', '起见', '岂但', '恰恰相反', '前后', '前者', '且', '然而', '然后', '然则', '让', '人家', '任', '任何', '任凭', '如', '如此', '如果', '如何', '如其', '如若', '如上所述', '若', '若非', '若是', '啥', '上下', '尚且', '设若', '设使', '甚而', '甚么', '甚至', '省得', '时候', '什么', '什么样', '使得', '是', '是的', '首先', '谁', '谁知', '顺', '顺着', '似的', '虽', '虽然', '虽说', '虽则', '随', '随着', '所', '所以', '他', '他们', '他人', '它', '它们', '她', '她们', '倘', '倘或', '倘然', '倘若', '倘使', '腾', '替', '通过', '同', '同时', '哇', '万一', '往', '望', '为', '为何', '为了', '为什么', '为着', '喂', '嗡嗡', '我', '我们', '呜', '呜呼', '乌乎', '无论', '无宁', '毋宁', '嘻', '吓', '相对而言', '像', '向', '向着', '嘘', '呀', '焉', '沿', '沿着', '要', '要不', '要不然', '要不是', '要么', '要是', '也', '也罢', '也好', '一', '一般', '一旦', '一方面', '一来', '一切', '一样', '一则', '依', '依照', '矣', '以', '以便', '以及', '以免', '以至', '以至于', '以致', '抑或', '因', '因此', '因而', '因为', '哟', '用', '由', '由此可见', '由于', '有', '有的', '有关', '有些', '又', '于', '于是', '于是乎', '与', '与此同时', '与否', '与其', '越是', '云云', '哉', '再说', '再者', '在', '在下', '咱', '咱们', '则', '怎', '怎么', '怎么办', '怎么样', '怎样', '咋', '照', '照着', '者', '这', '这边', '这儿', '这个', '这会儿', '这就是说', '这里', '这么', '这么点儿', '这么些', '这么样', '这时', '这些', '这样', '正如', '吱', '之', '之类', '之所以', '之一', '只是', '只限', '只要', '只有', '至', '至于', '诸位', '着', '着呢', '自', '自从', '自个儿', '自各儿', '自己', '自家', '自身', '综上所述', '总的来看', '总的来说', '总的说来', '总而言之', '总之', '纵', '纵令', '纵然', '纵使', '遵照', '作为', '兮', '呃', '呗', '咚', '咦', '喏', '啐', '喔唷', '嗬', '嗯', '嗳']\n"
     ]
    }
   ],
   "source": [
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
