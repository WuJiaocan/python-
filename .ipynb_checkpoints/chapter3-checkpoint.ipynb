{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于规则的分词：正/逆/双向最大匹配，依赖词典，新出现的词（未登陆词）可能无法分出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南京市', '长江大桥']\n"
     ]
    }
   ],
   "source": [
    "# 逆向最大匹配\n",
    "class IMM(object):\n",
    "    \n",
    "    def __init__(self, dic_path):\n",
    "        self.dictionary = set() # 集合，添加用add()方法\n",
    "        self.maximun = 0\n",
    "        # 读取词典\n",
    "        with open(dic_path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                if len(line) > self.maximun:\n",
    "                    self.maximun = len(line)\n",
    "                    \n",
    "    def cut(self, text):\n",
    "        result = []\n",
    "        index = len(text)  # 获取当前text的长度，从后往前定位到最后一个字\n",
    "        while index > 0:\n",
    "            word = None    # 一开始word里是空的\n",
    "            for size in range(self.maximun, 0, -1):  # 逆向往前推\n",
    "                if index - size < 0:   # 按照词典里最长的词语的长度来首次匹配，若匹配不上，则退出\n",
    "                    continue\n",
    "                piece = text[(index-size): index] #  若逆向且最大长度的词语的长度符合要求\n",
    "                if piece in self.dictionary:      #  且该词语也存在词典中\n",
    "                    word = piece\n",
    "                    result.append(word)\n",
    "                    index -= size\n",
    "                    break\n",
    "            if word is None:\n",
    "                index -= 1\n",
    "        return result[::-1] # 逆向添加的结果，最后要逆向输出\n",
    "\n",
    "def main():\n",
    "    text = \"南京市长江大桥\"\n",
    "    tokenizer = IMM(\"./chapter3/imm_dic.utf8\")\n",
    "    print(tokenizer.cut(text))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    词典imm_dic.utf8中的内容\n",
    "'''\n",
    "# imm_dic.utf8 \n",
    "# 南京市\n",
    "# 南京市长\n",
    "# 长江大桥\n",
    "# 人名解放军\n",
    "# 大桥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于统计的分词 HMM CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合分词：规则＋统计，如jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高频词提取1\n",
    "\n",
    "def get_content(path):\n",
    "    with open(path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "        content = ''\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "        return content\n",
    "\n",
    "def get_TF(words, topK=10):\n",
    "    tf_dic = {}\n",
    "    for w in words:\n",
    "        tf_dic[w] = tf_dic.get(w, 0) + 1\n",
    "#         print(tf_dic)\n",
    "    return sorted(tf_dic.items(), key=lambda x: x[1], reverse=True)[:topK]\n",
    "\n",
    "def stop_words(path):\n",
    "    with open(path) as f:\n",
    "        return [l.strip() for l in f]\n",
    "    \n",
    "# stop_words('./chapter3/stop_words.utf8')\n",
    "\n",
    "# 分词\n",
    "# def main():\n",
    "#     import glob\n",
    "#     import random\n",
    "#     import jieba\n",
    "    \n",
    "#     files = glob.glob(\"./chapter3/data/C000013/*.txt\") # 查找符合特定规则的文件路径名, \"*\"匹配0个或多个字符；\"?\"匹配单个字符；\"[]\"匹配指定范围内的字符,如：[0-9]匹配数字\n",
    "#     corpus = [get_content(x) for x in files[:5]]\n",
    "# #     print(files[:5])\n",
    "# #     print(corpus)\n",
    "    \n",
    "#     sample_inx = random.randint(0, len(corpus))\n",
    "# #     print(len(corpus))\n",
    "# #     print(sample_inx)\n",
    "\n",
    "    \n",
    "#     import jieba.posseg as psg\n",
    "    \n",
    "#     split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./chapter3/stop_words.utf8')]\n",
    "    \n",
    "# #     print(corpus[sample_inx])\n",
    "    \n",
    "#     print('样本之一：'+ corpus[sample_inx])\n",
    "#     print('样本分词效果：'+'/ '.join(split_words))\n",
    "#     print('样本的topK（10）词：'+ str(get_TF(split_words)))\n",
    "\n",
    "# main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "6\n",
      "['日本', '医科大学', '医院', '疫苗', '方法', '乙型肝炎', '母婴', '方法', '次数', '效果', '优点', '日本', '经济', '新闻', '医院', '院长', '稻叶', '宪', '新', '方法', '婴儿', '药物', '婴儿', '小时', '乙肝', '专题', '疫苗', '婴儿', '健康检查', '时', '疫苗', '证实', '方法', '婴儿', '免疫力', '乙型肝炎', '肝硬化', '肝癌', '专题', '血液', '母婴', '传统', '疫苗', '病毒', '母亲', '新生儿', '次数', '周期长', '婴儿', '现象', '乙肝', '母婴', '病例', '中约', '疫苗']\n",
      "***********************************************************************************\n",
      "['ns', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'n', 'n', 'n', 'n', 'n', 'nr', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'n']\n",
      "样本之一：日本独协医科大学医院开发一种新的疫苗接种方法，可有效阻断乙型肝炎在母婴间的传播。新方法具有接种次数少、效果好的优点。据《日本经济新闻》报道，这家医院的院长稻叶宪之等人开发的新方法分两次给婴儿注射药物，第一次是在婴儿出生24小时内立即注射球蛋白和乙肝(专题 访谈 咨询)疫苗，第二次是在婴儿出生一个月后接受健康检查时，再注射一次疫苗。临床研究证实，这种方法可使婴儿获得足够的免疫力。乙型肝炎可发展为肝硬化及肝癌(专题 访谈 咨询)等。它通常经血液传播或在母婴间传播。传统的疫苗接种法虽然也能阻断病毒在母亲和新生儿之间传播，但接种次数多，且周期长，因而经常出现婴儿未及时接种的现象。调查显示，乙肝在母婴间传播的病例中约有30％是因未按规定及时注射疫苗引起的。\n",
      "样本分词效果：日本/ 医科大学/ 医院/ 疫苗/ 方法/ 乙型肝炎/ 母婴/ 方法/ 次数/ 效果/ 优点/ 日本/ 经济/ 新闻/ 医院/ 院长/ 稻叶/ 宪/ 新/ 方法/ 婴儿/ 药物/ 婴儿/ 小时/ 乙肝/ 专题/ 疫苗/ 婴儿/ 健康检查/ 时/ 疫苗/ 证实/ 方法/ 婴儿/ 免疫力/ 乙型肝炎/ 肝硬化/ 肝癌/ 专题/ 血液/ 母婴/ 传统/ 疫苗/ 病毒/ 母亲/ 新生儿/ 次数/ 周期长/ 婴儿/ 现象/ 乙肝/ 母婴/ 病例/ 中约/ 疫苗\n",
      "样本的topK（10）词：[('疫苗', 5), ('婴儿', 5), ('方法', 4), ('母婴', 3), ('日本', 2), ('医院', 2), ('乙型肝炎', 2), ('次数', 2), ('乙肝', 2), ('专题', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#高频词提取2\n",
    "\n",
    "def main():\n",
    "    import glob\n",
    "    import random\n",
    "    import jieba\n",
    "    import jieba.posseg as psg\n",
    "    \n",
    "    files = glob.glob('./chapter3/data/C000013/*.txt')\n",
    "    corpus = [get_content(x) for x in files]  # 若不切片，就读取设置路径下的全部文件\n",
    "    print(len(corpus))   # 15\n",
    "#     print(corpus)\n",
    "    \n",
    "    sample_inx = random.randint(0, len(corpus))\n",
    "    print(sample_inx)\n",
    "#     sample_inx = 3\n",
    "    \n",
    "    aa = [w for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    \n",
    "    bb = [t for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    \n",
    "    print(aa)\n",
    "    print(\"***********************************************************************************\")\n",
    "    print(bb)\n",
    "    \n",
    "    split_words = [w for w, t in psg.cut(corpus[sample_inx]) \n",
    "                   if w not in stop_words('./chapter3/stop_words.utf8') and t.startswith('n')]\n",
    "    print('样本之一：' + corpus[sample_inx])\n",
    "    print('样本分词效果：' + '/ '.join(split_words))\n",
    "    print('样本的topK（10）词：'+ str(get_TF(split_words)))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文/nz 分词/n 是/v 文本处理/n 不可或缺/l 的/uj 一步/m ！/x\n"
     ]
    }
   ],
   "source": [
    "# 练习jieba的使用\n",
    "\n",
    "import jieba.posseg as psg\n",
    "\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "\n",
    "seg_list = psg.cut(sent)\n",
    "\n",
    "print(' '.join(['{0}/{1}'.format(w, t) for w, t in seg_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大波浪 10\n",
      "\n",
      "jieba分词 n\n",
      "\n",
      "金融词典 7  \n",
      "\n",
      "加载词典前: jieba分词/ 非常/ 好用/ ，/ 可以/ 自定义/ 金融词典/ ！\n",
      "加载词典后: jieba分词/ 非常/ 好用/ ，/ 可以/ 自定义/ 金融词典/ ！\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('./chapter3/dict.txt.big') # 改变主词典的路径，默认用的是jieba的分词词典\n",
    "\n",
    "with open('./chapter3/user_dict.utf8', 'r') as f:\n",
    "    for l in f:\n",
    "        print(l)\n",
    "        \n",
    "sent = 'jieba分词非常好用，可以自定义金融词典！'\n",
    "seg_list = jieba.cut(sent)\n",
    "print('加载词典前:', '/ '.join(seg_list))\n",
    "\n",
    "jieba.load_userdict('./chapter3/user_dict.utf8') # 添加一些用户自定义的词典\n",
    "seg_list = jieba.cut(sent)\n",
    "print('加载词典后:', '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好丑/ 的/ 证件/ 照片\n",
      "好丑/ 的/ 证件照片\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "sent = '好丑的证件照片'\n",
    "print('/ '.join(jieba.cut(sent, HMM=False)))\n",
    "\n",
    "jieba.suggest_freq(('证件照片'), True)\n",
    "print('/ '.join(jieba.cut(sent, HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言 2.0790900005043476\n",
      "NLP 0.5197725001260869\n",
      "计算机 0.5197725001260869\n",
      "领域 0.5197725001260869\n",
      "人机交互 0.5197725001260869\n",
      "挑战 0.5197725001260869\n",
      "理解 0.5197725001260869\n",
      "处理 0.4705091875965217\n",
      "涉及 0.3839134341652174\n",
      "人工智能 0.25988625006304344\n"
     ]
    }
   ],
   "source": [
    "# 基于 TF-IDF 算法的关键词抽取 \n",
    "# jieba.analyse.extract_tags()\n",
    "\n",
    "import jieba.analyse as aly\n",
    "\n",
    "content = '''\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "\n",
    "'''\n",
    "#加载自定义idf词典\n",
    "aly.set_idf_path('./chapter3/idf.txt.big')\n",
    "#加载停用词典\n",
    "aly.set_stop_words('./chapter3/stop_words.utf8')\n",
    "\n",
    "# 第一个参数：待提取关键词的文本\n",
    "# 第二个参数：返回关键词的数量，重要性从高到低排序\n",
    "# 第三个参数：是否同时返回每个关键词的权重\n",
    "# 第四个参数：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词\n",
    "keywords = aly.extract_tags(content, topK=10, withWeight=True, allowPOS=())\n",
    "\n",
    "for item in keywords:\n",
    "    # 分别为关键词和相应的权重\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "涉及 1.0\n",
      "计算机 0.9618169519358478\n",
      "处理 0.8124660402732825\n",
      "理解 0.7885898958379202\n",
      "挑战 0.7833575495518058\n",
      "人机交互 0.7343470452632993\n",
      "语言学 0.727536034596871\n",
      "人类 0.6290562193534068\n",
      "人工智能 0.5809911385488661\n",
      "关注 0.577881611632419\n"
     ]
    }
   ],
   "source": [
    "# 基于 TextRank 算法的关键词抽取\n",
    "# jieba.analyse.TextRank() \n",
    "import jieba.analyse as aly\n",
    "\n",
    "content = '''\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n",
    "因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。\n",
    "在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n",
    "'''\n",
    "\n",
    "# 第一个参数：待提取关键词的文本\n",
    "# 第二个参数：返回关键词的数量，重要性从高到低排序\n",
    "# 第三个参数：是否同时返回每个关键词的权重\n",
    "# 第四个参数：词性过滤，为空表示过滤所有，与TF—IDF不一样！\n",
    "\n",
    "keywords = jieba.analyse.textrank(content, topK=10, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "for item in keywords:\n",
    "    # 分别为关键词和相应的权重\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
