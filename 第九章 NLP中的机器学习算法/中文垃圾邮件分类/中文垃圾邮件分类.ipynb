{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    type()查看变量类型，比如array/list/dataframe\n",
    "    .dtype查看构成变量的元素的类型，比如float/int\n",
    "    .astype修改构成变量的元素的类型，比如从float改成int\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "    tolist()可以用于np.narray或者pd.series转换成list\n",
    "'''\n",
    "\n",
    "'''\n",
    "    np.round(number,int) number四舍五入保留int位小数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujiaocan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "    获取数据\n",
    "    :return: 文本数据，对应的labels\n",
    "    '''\n",
    "    with open(\"data/ham_data.txt\", encoding=\"utf8\") as ham_f, open(\"data/spam_data.txt\", encoding=\"utf8\") as spam_f:\n",
    "        ham_data = ham_f.readlines()\n",
    "        spam_data = spam_f.readlines()\n",
    "\n",
    "        ham_label = np.ones(len(ham_data)).tolist()     # 正样本\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()  # 负样本\n",
    "\n",
    "        corpus = ham_data + spam_data\n",
    "\n",
    "        labels = ham_label + spam_label\n",
    "\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    '''\n",
    "    \n",
    "    :param corpus: 文本数据\n",
    "    :param labels: label数据\n",
    "    :param test_data_proportion:测试数据占比 \n",
    "    :return: 训练数据,测试数据，训练label,测试label\n",
    "    '''\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n",
    "                                                        test_size=test_data_proportion, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('准确率:', np.round(metrics.accuracy_score(true_labels,predicted_labels),2))\n",
    "    print('精度:', np.round(metrics.precision_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print('召回率:', np.round(metrics.recall_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print('F1得分:', np.round(metrics.f1_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print(\"*************************************************************\")\n",
    "    \n",
    "\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels, test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    corpus, labels = get_data()  # 获取数据集\n",
    "    print(\"总的数据量:\", len(labels))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    corpus, labels = remove_empty_docs(corpus, labels)\n",
    "    print(\"移除空样本之后总的数据量：\", len(labels))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('样本之一:', corpus[10])\n",
    "    print('样本的label:', labels[10])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    label_name_map = [\"垃圾邮件\", \"正常邮件\"] # 0 垃圾邮件； 1 正常邮件\n",
    "    print('实际类型:', label_name_map[int(labels[10])], label_name_map[int(labels[5900])])\n",
    "\n",
    "    # 对数据进行划分\n",
    "    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus, labels, test_data_proportion=0.3)\n",
    "\n",
    "    from normalization import normalize_corpus\n",
    "\n",
    "    # 进行归一化\n",
    "    norm_train_corpus = normalize_corpus(train_corpus) # 分词，去特殊符号，去停用词\n",
    "    norm_test_corpus = normalize_corpus(test_corpus)\n",
    "#     print(\"训练集之一：\", norm_train_corpus[0])\n",
    "#     print(\"测试集之一：\" ,norm_test_corpus[0])\n",
    "#     print(\"\\n\")\n",
    "\n",
    "#     ''.strip()\n",
    "\n",
    "    from feature_extractors import bow_extractor, tfidf_extractor\n",
    "    import gensim\n",
    "    import jieba\n",
    "\n",
    "    # 词袋模型特征\n",
    "    bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "    bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "    # tfidf 特征\n",
    "    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "    tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "    # tokenize documents\n",
    "    tokenized_train = [jieba.lcut(text) for text in norm_train_corpus]\n",
    "    print(\"tokenized_train[2]：\" ,tokenized_train[2])\n",
    "    tokenized_test = [jieba.lcut(text) for text in norm_test_corpus]\n",
    "    \n",
    "#     # build word2vec 模型\n",
    "#     model = gensim.models.Word2Vec(tokenized_train, size=500, window=100, min_count=30, sample=1e-3)\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    mnb = MultinomialNB()\n",
    "    svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # 基于词袋模型的多项朴素贝叶斯\n",
    "    print(\"基于词袋模型特征的贝叶斯分类器\")\n",
    "    mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                       train_features=bow_train_features,\n",
    "                                                       train_labels=train_labels,\n",
    "                                                       test_features=bow_test_features,\n",
    "                                                       test_labels=test_labels)\n",
    "\n",
    "    # 基于词袋模型特征的逻辑回归\n",
    "    print(\"基于词袋模型特征的逻辑回归\")\n",
    "    lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                      train_features=bow_train_features,\n",
    "                                                      train_labels=train_labels,\n",
    "                                                      test_features=bow_test_features,\n",
    "                                                      test_labels=test_labels)\n",
    "\n",
    "    # 基于词袋模型的支持向量机方法\n",
    "    print(\"基于词袋模型的支持向量机\")\n",
    "    svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                       train_features=bow_train_features,\n",
    "                                                       train_labels=train_labels,\n",
    "                                                       test_features=bow_test_features,\n",
    "                                                       test_labels=test_labels)\n",
    "\n",
    "\n",
    "    # 基于tfidf的多项式朴素贝叶斯模型\n",
    "    print(\"基于tfidf的贝叶斯模型\")\n",
    "    mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "    # 基于tfidf的逻辑回归模型\n",
    "    print(\"基于tfidf的逻辑回归模型\")\n",
    "    lr_tfidf_predictions=train_predict_evaluate_model(classifier=lr,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "\n",
    "\n",
    "    # 基于tfidf的支持向量机模型\n",
    "    print(\"基于tfidf的支持向量机模型\")\n",
    "    svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量: 10001\n",
      "\n",
      "\n",
      "移除空样本之后总的数据量： 10001\n",
      "\n",
      "\n",
      "样本之一: 北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "样本的label: 1.0\n",
      "\n",
      "\n",
      "实际类型: 正常邮件 垃圾邮件\n",
      "tokenized_train[2]： ['10156', '说', '的', '呵呵', '标题', 'Re', '我', '要是', '你', '女朋友', '绝对', '跟', '你', '分手', 'Re', '昨晚', '猫', '又', '闹', '了', '一夜', '嗯', '，', '谁', '说', '我', '养猫', '是因为', '有', '爱心', '我', '跟', '谁', '急', '是', '哈', '，', '不是', '用', '爱心', '区分', '的', '喜欢', '宠物', '的', '就', '养', '，', '不', '喜欢', '的', '就', '不养', '呗', '卷', '卷', '，', '你', '搞', '成', '这', '副', '鬼', '样子', '，', '还', '好意思', '来', '找', '我', '撒娇', '。', '。', '。']\n",
      "基于词袋模型特征的贝叶斯分类器\n",
      "准确率: 0.79\n",
      "精度: 0.85\n",
      "召回率: 0.79\n",
      "F1得分: 0.78\n",
      "*************************************************************\n",
      "基于词袋模型特征的逻辑回归\n",
      "准确率: 0.96\n",
      "精度: 0.96\n",
      "召回率: 0.96\n",
      "F1得分: 0.96\n",
      "*************************************************************\n",
      "基于词袋模型的支持向量机\n",
      "准确率: 0.97\n",
      "精度: 0.97\n",
      "召回率: 0.97\n",
      "F1得分: 0.97\n",
      "*************************************************************\n",
      "基于tfidf的贝叶斯模型\n",
      "准确率: 0.79\n",
      "精度: 0.85\n",
      "召回率: 0.79\n",
      "F1得分: 0.78\n",
      "*************************************************************\n",
      "基于tfidf的逻辑回归模型\n",
      "准确率: 0.94\n",
      "精度: 0.94\n",
      "召回率: 0.94\n",
      "F1得分: 0.94\n",
      "*************************************************************\n",
      "基于tfidf的支持向量机模型\n",
      "准确率: 0.97\n",
      "精度: 0.97\n",
      "召回率: 0.97\n",
      "F1得分: 0.97\n",
      "*************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bb9a518fe1a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_tfidf_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"邮件类型：\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_name_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    显示部分正确归类和部分错分的邮件\n",
    "'''\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 0:\n",
    "        print(\"邮件类型：\", label_name_map[int(label)])\n",
    "        print(\"预测的邮件类型：\", label_name_map[int(predicted_label)])\n",
    "        print(\"文本：-\")\n",
    "        print(re.sub(\"\\n\", \" \", document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是个测试句子'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = \"这是个测试a句子\"\n",
    "re.sub(\"a\", \"\", aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
